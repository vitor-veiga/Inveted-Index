import type { Document } from '../lib/inverted-index/types';

export const sampleDocuments: Document[] = [
  {
    id: 'doc1',
    title: 'Introduction to Data Structures',
    content: `Data structures are fundamental components in computer science that organize and store data efficiently. Understanding data structures is essential for writing efficient code and solving complex problems. This document introduces core concepts, comparing trade-offs between memory use, operation speed, and implementation complexity. Examples such as arrays, linked lists, stacks, queues, trees, and graphs are discussed with common use-cases and performance considerations. Readers will gain intuition for picking the right structure for common tasks and how these choices affect algorithms.`,
    wordCount: 0,
  },
  {
    id: 'doc2',
    title: 'Search Engines and Information Retrieval',
    content: `Modern search engines rely on sophisticated data structures and ranking algorithms to deliver fast, relevant results at scale. The inverted index maps terms to documents and enables quick lookups across large corpora; additional data such as term positions and frequencies support phrase queries and ranking features. Systems also incorporate tokenization, normalization, and query expansion to handle natural language variations. Practical considerations include indexing pipelines, sharding, and trade-offs between latency and freshness when updating indexes.`,
    wordCount: 0,
  },
  {
    id: 'doc3',
    title: 'Hash Tables and Applications',
    content: `Hash tables use a hash function to map keys to indices, providing average O(1) lookup time for inserts, deletes, and lookups in typical cases. Robust systems choose hash functions carefully and implement collision resolution strategies such as chaining or open addressing to maintain performance under load. Real-world implementations also handle resizing, memory allocation, and concurrency concerns to avoid pathological slowdowns. Use cases range from symbol tables in compilers to caches and in-memory key-value stores.`,
    wordCount: 0,
  },
  {
    id: 'doc4',
    title: 'Arrays vs Linked Lists',
    content: `Arrays offer constant-time random access and compact memory layouts, making them ideal for read-heavy workloads where indices are known. Linked lists provide efficient insertions and deletions at arbitrary positions without moving other elements, which can be beneficial for certain streaming or splice-heavy operations. However, linked lists have poorer cache locality and higher per-node overhead. Choosing between them depends on access patterns, memory constraints, and the cost of element shifting versus pointer overhead.`,
    wordCount: 0,
  },
  {
    id: 'doc5',
    title: 'Stacks and Queues',
    content: `Stacks follow LIFO (last-in-first-out) semantics and are useful for function call management, depth-first traversals, and undo operations. Queues follow FIFO (first-in-first-out) and are commonly used in scheduling, breadth-first searches, and producer-consumer scenarios. Both structures can be implemented with arrays or linked lists depending on expected workloads and memory characteristics. Variants such as double-ended queues (deques) and priority queues extend functionality for more nuanced use-cases.`,
    wordCount: 0,
  },
  {
    id: 'doc6',
    title: 'Trees: Binary and Beyond',
    content: `Trees are hierarchical structures that organize data in parent-child relationships; binary trees limit nodes to two children and are the foundation of many search and expression representations. Balanced variants such as AVL and red-black trees enforce structural invariants to guarantee logarithmic-time operations for search, insert, and delete. Other tree types (B-trees, tries, suffix trees) target specific storage or retrieval patterns and perform better for large datasets or prefix-based operations. Understanding traversal orders and tree rotations is key to implementing and debugging these structures.`,
    wordCount: 0,
  },
  {
    id: 'doc7',
    title: 'Graphs and Traversal',
    content: `Graphs model pairwise relationships between entities and can be directed or undirected, weighted or unweighted. Traversal algorithms such as depth-first search (DFS) and breadth-first search (BFS) form the basis for many graph analyses including connectivity, cycle detection, and shortest-path algorithms when combined with weights. Algorithms like Dijkstra, Bellman-Ford, and A* address pathfinding in weighted graphs, while strongly connected components, topological sort, and minimum spanning tree algorithms serve other analytical needs. Representations (adjacency lists vs matrices) impact algorithmic performance and memory usage.`,
    wordCount: 0,
  },
  {
    id: 'doc8',
    title: 'Sorting Algorithms Overview',
    content: `Sorting algorithms arrange elements in a defined order and are central to many higher-level algorithms. Quicksort is often fast in practice but has worst-case quadratic time without precautions; mergesort guarantees O(n log n) time and is stable, making it suitable for external sorting. Heapsort provides O(n log n) worst-case time with in-place storage, while simple algorithms like insertion sort perform well on small or nearly sorted datasets. Algorithm choice depends on stability, memory constraints, and distribution of input data.`,
    wordCount: 0,
  },
  {
    id: 'doc9',
    title: 'Searching: Linear and Binary',
    content: `Linear search scans elements sequentially and is useful for small or unsorted collections where simplicity matters. Binary search requires a sorted collection and delivers O(log n) time by repeatedly halving the search range, making it highly efficient for lookups on ordered arrays. For other structures such as trees or hash tables, different lookup strategies apply based on the underlying organization. Consider the cost of maintaining sorted order versus faster lookup when picking a strategy.`,
    wordCount: 0,
  },
  {
    id: 'doc10',
    title: 'Complexity Notation',
    content: `Big O notation describes upper bounds of algorithmic complexity and abstracts away constant factors to compare algorithms by growth rates. Complementary notations like Theta (Θ) and Omega (Ω) describe tight bounds and lower bounds respectively. Complexity analysis considers both time and space, and distinguishes worst-case, average-case, and amortized complexities to guide practical algorithm selection. Understanding how input distributions and constants affect real-world performance is crucial beyond asymptotic analysis.`,
    wordCount: 0,
  },
  {
    id: 'doc11',
    title: 'Inverted Index Fundamentals',
    content: `An inverted index maps each term to a posting list of documents where the term appears; it is the core data structure behind many search systems. Posting lists may include additional metadata like term frequency and positions to support ranking, phrase queries, and proximity search. Building an inverted index requires tokenization, normalization, and sometimes filtering of stop-words or rare terms. Efficient storage, merging, and compression strategies enable large-scale indexes to be queried quickly.`,
    wordCount: 0,
  },
  {
    id: 'doc12',
    title: 'Tokenization and Normalization',
    content: `Tokenization splits raw text into discrete terms or tokens and is sensitive to language-specific rules, punctuation, and punctuation-preserving needs. Normalization techniques such as lowercasing, stemming, lemmatization, and stop-word removal reduce term variation and help improve match rates while reducing index size. Additional steps like synonym expansion or phrase detection can further enrich query matching. Balancing recall and precision is an important design decision when tuning preprocessing steps.`,
    wordCount: 0,
  },
  {
    id: 'doc13',
    title: 'TF and IDF',
    content: `Term Frequency (TF) measures how often a term appears in a document and reflects the term's importance within that document. Inverse Document Frequency (IDF) downweights terms that appear frequently across many documents, as they tend to be less discriminative. TF-IDF combines both signals to score terms for ranking, providing a simple yet effective measure of relevance in many retrieval tasks. Modern systems often augment TF-IDF with additional features and learning-to-rank models for improved results.`,
    wordCount: 0,
  },
  {
    id: 'doc14',
    title: 'Boolean Queries',
    content: `Boolean queries use logical operators like AND, OR, and NOT to combine terms and produce precise result sets. Efficient evaluation typically merges posting lists (intersection for AND, union for OR) and may use skip pointers or heuristics to accelerate operations on long lists. While Boolean retrieval provides exact control, it lacks ranking; hybrid systems often combine Boolean filtering with ranking models to present the most relevant matches first.`,
    wordCount: 0,
  },
  {
    id: 'doc15',
    title: 'Phrase and Proximity Search',
    content: `Phrase search matches exact sequences of terms and requires positional information in posting lists to verify contiguous occurrences. Proximity search generalizes this by finding terms within a specified distance, enabling flexible matching for near-phrases and related concepts. Storing term positions increases index size but greatly enhances query expressiveness for linguistic queries. Use cases include exact phrase matching, highlighted snippets, and contextual relevance scoring.`,
    wordCount: 0,
  },
  {
    id: 'doc16',
    title: 'Index Compression',
    content: `Index compression reduces storage and I/O costs for posting lists and can dramatically speed up query processing by improving cache and disk utilization. Techniques include delta or gap encoding of sorted document IDs, variable-byte or gamma coding for compact integers, and block-based compression for SIMD-friendly decompression. Choosing the right scheme balances CPU cost for decompression with I/O savings. In practice, hybrid strategies are used to get the best trade-offs for given workloads.`,
    wordCount: 0,
  },
  {
    id: 'doc17',
    title: 'Caching and Performance',
    content: `Caching frequently accessed posting lists, term statistics, and even full query results reduces latency and backend load in search systems. Multi-tier caches (in-memory, SSD, distributed caches) serve different performance and capacity needs. Effective cache eviction policies and awareness of query distributions ensure high hit rates. Monitoring and tuning caches based on real traffic patterns yields large improvements in user-facing latency.`,
    wordCount: 0,
  },
  {
    id: 'doc18',
    title: 'Distributed Indexing',
    content: `Large-scale search engines partition indexes across shards to distribute storage and query workload; shards can be assigned by document ranges, hashing, or routing rules. Replication provides fault tolerance and improves query throughput by distributing queries across replicas. Challenges include balancing shards, handling re-sharding during growth, and maintaining consistency during updates. Distributed indexing architectures also need efficient routing and query merging strategies to provide low-latency results.`,
    wordCount: 0,
  },
  {
    id: 'doc19',
    title: 'Ranking with Learning-to-Rank',
    content: `Learning-to-rank applies machine learning models to combine multiple features (textual relevance, click data, freshness, popularity) into a single ranking score. Models range from pointwise and pairwise learners to listwise approaches, and are trained on labeled relevance judgments or user interaction logs. Careful feature engineering, regularization, and offline evaluation are needed to avoid overfitting and to ensure generalization. Online A/B testing validates production improvements and safety.`,
    wordCount: 0,
  },
  {
    id: 'doc20',
    title: 'Natural Language Processing Basics',
    content: `NLP techniques such as tokenization, part-of-speech tagging, and named entity recognition enrich text representations and allow more informed matching and extraction. Modern pipelines may include language detection, sentence segmentation, dependency parsing, and semantic role labeling to support advanced features. Pretrained embeddings and transformer-based models provide powerful semantic signals that complement traditional lexical signals. Combining symbolic and neural approaches can yield robust systems for real-world text.`,
    wordCount: 0,
  },
  {
    id: 'doc21',
    title: 'Stemming vs Lemmatization',
    content: `Stemming heuristically trims words to root forms and is fast but sometimes over-aggressive, producing non-dictionary stems. Lemmatization uses morphological analysis and vocabulary to produce accurate base forms suitable for precise matching. The choice depends on language complexity and the need for precise normalization; lemmatization is often preferred when accuracy outweighs preprocessing cost. Both techniques reduce vocabulary size and improve matching across inflected forms.`,
    wordCount: 0,
  },
  {
    id: 'doc22',
    title: 'Analytics and Query Logs',
    content: `Analyzing query logs provides insights into user behavior, common intents, and failure modes that can guide ranking improvements and UX changes. Query analytics support features like autocomplete, query suggestion, and personalization by surfacing frequent or trending queries. Privacy and sampling strategies are important when working with real user data to avoid exposing sensitive information. Longitudinal analysis helps track the impact of ranking changes and search quality over time.`,
    wordCount: 0,
  },
  {
    id: 'doc23',
    title: 'Spell Correction and Suggestions',
    content: `Spell correction and suggestion systems help users recover from typos and ambiguous queries using techniques from edit-distance heuristics to probabilistic language models. Candidate generation often uses n-gram indexes or character-based transformations, while ranking relies on corpus statistics and context-aware models. Suggestion interfaces should present corrections confidently and allow users to revert to the original query. Evaluating suggestion quality involves both offline metrics and user engagement signals.`,
    wordCount: 0,
  },
  {
    id: 'doc24',
    title: 'Multilingual Search',
    content: `Multilingual search requires language-specific tokenization, normalization, and stop-word handling, plus reliable language detection for documents and queries. Cross-lingual retrieval and translation-based approaches enable searching across languages, while language-aware ranking improves relevance for localized content. Managing multilingual resources and evaluation datasets adds operational complexity. Real-world deployments often combine language models, translation pipelines, and locale-sensitive relevance adjustments.`,
    wordCount: 0,
  },
  {
    id: 'doc25',
    title: 'Index Maintenance',
    content: `Index maintenance involves merging index segments to control file counts and reclaim space, handling deletes and updates efficiently, and optimizing structures for read-heavy workloads. Scheduling merges, throttling background work, and balancing disk I/O against indexing latency are key operational concerns. Providing near-real-time updates while keeping query latency low requires careful engineering and instrumentation. Tools and observability play a crucial role in maintaining index health.`,
    wordCount: 0,
  },
  {
    id: 'doc26',
    title: 'Real-time Indexing',
    content: `Real-time indexing aims to make newly added documents searchable with minimal delay while balancing throughput and query latency. Approaches include in-memory buffers, nearline segments, and asynchronous merging to background storage. System designs must consider durability, replication, and consistency guarantees when exposing newly indexed content. Metrics such as index lag and tail latency help monitor real-time behavior.`,
    wordCount: 0,
  },
  {
    id: 'doc27',
    title: 'Vector Search and Embeddings',
    content: `Vector search uses dense embeddings and approximate nearest neighbor search to retrieve semantically similar items and complements traditional keyword-based retrieval. Embeddings capture latent semantics from neural models and enable matching by meaning rather than exact lexical overlap. Efficient index structures (HNSW, IVF, PQ) and quantization techniques make large-scale vector search feasible. Hybrid approaches combine lexical and vector signals to benefit from both precision and semantic recall.`,
    wordCount: 0,
  },
  {
    id: 'doc28',
    title: 'Privacy and Security',
    content: `Privacy-preserving search and secure storage are essential for protecting sensitive user data and complying with regulations like GDPR. Techniques include data anonymization, access controls, encryption at rest and in transit, and query logging policies that minimize sensitive retention. Building user trust also requires transparent data practices and the ability to fulfill data subject requests. Security considerations extend to protecting model inputs and preventing data leakage in ranking pipelines.`,
    wordCount: 0,
  },
  {
    id: 'doc29',
    title: 'Benchmarks and Evaluation',
    content: `Benchmarks assess throughput, latency, and relevance to quantify search system performance under realistic workloads. Common relevance metrics include precision, recall, mean reciprocal rank (MRR), and normalized discounted cumulative gain (nDCG). Operational metrics such as queries per second, tail latencies, and resource consumption guide capacity planning. Well-designed benchmarks combine offline evaluation with live experiments to validate user-facing impact.`,
    wordCount: 0,
  },
  {
    id: 'doc30',
    title: 'Future of Search',
    content: `The future of search blends large language models, multimodal understanding, and tighter integration of semantic and lexical retrieval techniques. Advances in retrieval-augmented generation, conversational search, and personalized ranking are reshaping how users interact with information. Ethical considerations, resource efficiency, and robustness remain central as systems get more capable and widely used. Researchers and engineers will continue combining classic IR techniques with modern machine learning to meet evolving user needs.`,
    wordCount: 0,
  },
];

export const exampleQueries = [
  'data structures',
  'hash tables',
  'search engines',
  'algorithms',
  'inverted index',
  'TF-IDF',
  'collision resolution',
  'binary tree balancing',
  'graph traversal',
  'phrase search',
  'index compression',
  'real-time indexing',
  'vector search embeddings',
  'boolean search AND OR',
  'stemming vs lemmatization',
  'spell correction',
  'multilingual tokenization',
  'learning to rank',
  'cache posting lists',
  'distributed shards',
];
